Spark Streaming from API with Airflow, Kafka, Cassandra, Control Centre, and Schema Registry
Overview:
This project demonstrates the process of setting up a data pipeline for streaming data from an API into Apache Spark using various technologies including Airflow, Kafka, Cassandra, Control Centre, and Schema Registry. The pipeline ensures real-time processing and storage of data, providing scalability and fault-tolerance.

Steps to Setup and Run the Project:
Step 1: Environment Setup
Utilize Docker to pull images for Apache Kafka, Apache Cassandra, Apache Airflow, Confluent Control Centre, and any necessary dependencies.
Configure the Docker containers to communicate with each other effectively.
Step 2: Start Kafka and Cassandra Clusters
Start the Docker containers for Apache Kafka and Apache Cassandra to enable messaging and data storage functionalities.
Step 3: Define Airflow DAGs
Create Airflow DAGs to define the workflow for the data pipeline.
Define tasks for fetching data from the API, sending data to Kafka, processing data with Spark Streaming, and storing processed data in Cassandra.
Step 4: Configure Airflow
Configure Airflow to ensure sequential execution of tasks and handle dependencies between tasks.
Set up Airflow to schedule the pipeline execution as per requirements.
Step 5: Run the Pipeline
Initiate the data pipeline by starting Airflow.
Airflow will execute the defined tasks according to the configured schedule and dependencies.
Step 6: Monitor the Pipeline
Monitor the progress of the pipeline using Airflow UI to track task execution and dependencies.
Utilize Confluent Control Centre to monitor Kafka clusters and topics, ensuring smooth data flow.
Step 7: Access Processed Data
Processed data generated by the Spark streaming application can be accessed from the Cassandra database.
Analyze and utilize the data as per project requirements.
